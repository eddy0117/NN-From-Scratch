{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筆記\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def im2col(input_feat: np.ndarray, N, kh, kw, out_h, out_w, stride):\n",
    "    im2col_feat = []\n",
    "    for n in range(N):\n",
    "        for ih in range(out_h):\n",
    "            for iw in range(out_w):\n",
    "                im2col_feat.append(input_feat[n, :, stride * ih:stride * ih + kh, stride * iw:stride * iw + kw])\n",
    "                # each element -> (C, kh, kw)\n",
    "    # input_feat -> (N*out_h*out_w, C, kh, kw)\n",
    "\n",
    "    return np.array(im2col_feat).reshape(N * out_h * out_w, -1)\n",
    "\n",
    "def convolution(input_feat: np.ndarray, filter: np.ndarray, kh, kw, stride=1, padding=0, bias=None):\n",
    "    '''\n",
    "    input_feat: (N, C, H, W)\n",
    "    filter: (out_C, in_C, kH, kw)\n",
    "    bias: (out_C, 1)\n",
    "    '''\n",
    "    N, C, H, W = input_feat.shape\n",
    "    out_h = int((H - kh + 2 * padding) // stride) + 1\n",
    "    out_w = int((W - kw + 2 * padding) // stride) + 1\n",
    "    out_c = filter.shape[0]\n",
    "    \n",
    "    if padding:\n",
    "        input_feat = np.pad(input_feat, ((0, 0), (0, 0), (padding, padding), (padding, padding)), 'constant', constant_values=0)\n",
    "\n",
    "    im2col_feat = im2col(input_feat, N, kh, kw, out_h, out_w, stride)\n",
    "    # im2col -> (N*out_h*out_w, C*kh*kw)\n",
    "\n",
    "    filter = filter.reshape(out_c, -1)\n",
    "    # filter -> (out_c, C*kh*kw)\n",
    "\n",
    "    # w @ x.T\n",
    "    # w -> (out_c, C*kh*kw)\n",
    "    # x.T -> (C*kh*kw, N*out_h*out_w)\n",
    "    if isinstance(bias, np.ndarray):\n",
    "        out_feat = filter @ im2col_feat.T + bias\n",
    "    else:\n",
    "        out_feat = filter @ im2col_feat.T\n",
    "    # out_feat -> (out_c, N*out_h*out_w)\n",
    "    \n",
    "    # 直接將 (out_c, N*out_h*out_w) reshape 成 (N, out_c, out_h, out_w) 會產生順序錯亂\n",
    "    # 所以先將 (out_c, N*out_h*out_w) 拆成 (out_c, N, out_h, out_w) 後再 permute\n",
    "    # out_feat -> (N, out_c, out_h, out_w)\n",
    "    return out_feat.reshape(out_c, N, out_h, out_w).transpose(1, 0, 2, 3)\n",
    "    # return out_feat.T.reshape(N, out_h, out_w, out_c).transpose(0, 3, 1, 2)\n",
    "   \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# input feat (c, h, w) -> (3, 3, 3)\n",
    "# filter (out_c, in_c, kh, kw) -> (3, 3, 2, 2)\n",
    "# bias (out_c, 1)\n",
    "bs = 2\n",
    "feat_h = 3\n",
    "feat_w = 3\n",
    "kh = 2\n",
    "kw = 2\n",
    "in_c = 3\n",
    "out_c = 3\n",
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "input_feat = np.random.randint(0, 20, size=(bs, in_c, feat_h, feat_w)).astype(np.float32)\n",
    "filter = np.random.randint(0, 5, size=(out_c, in_c, kh, kw)).astype(np.float32)\n",
    "# bias = np.random.randint(0, 5, size=(2, 1))\n",
    "bias = np.zeros((out_c, 1), dtype=np.float32)\n",
    "\n",
    "out = convolution(input_feat, filter, kh=kh, kw=kw, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "# 與 pytorch 的實現方法對照結果\n",
    "out_t = F.conv2d(torch.tensor(input_feat), torch.tensor(filter), stride=stride, padding=padding, bias=torch.tensor(bias).squeeze(1))\n",
    "\n",
    "\n",
    "print(f'input feat:\\n{input_feat}')\n",
    "print('======================')\n",
    "print(f'filter\\n:{filter}')\n",
    "print('======================')\n",
    "print(f'my out:\\n{out}')\n",
    "print(f'torch out:\\n{out_t.numpy()}')\n",
    "np.allclose(out, out_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from my_mlp import MLP, CrossEntropyLoss\n",
    "from my_nn_lib import ReLU, Softmax, LeckyReLU, Linear, BaseModule, Conv2d, Flatten\n",
    "\n",
    "class MyModel(MLP):\n",
    "    def __init__(self, layer_list):\n",
    "        self.layers = layer_list\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def backward(self, X, label):\n",
    "\n",
    "        idx_maxlayer = len(self.layers)-1\n",
    "        dLdZ = label\n",
    "\n",
    "        for idx in range(idx_maxlayer, -1, -1):\n",
    "            # 倒敘遍歷\n",
    "\n",
    "            dLdZ = self.layers[idx].backward(delta=dLdZ)\n",
    "            \n",
    "    \n",
    "    def update_params(self, opt_params):\n",
    "        for layer in self.layers:\n",
    "            layer.update_params(opt_params)\n",
    "        \n",
    "    def get_pred(self, X, with_onehot=False):\n",
    "        pred = self.forward(X)\n",
    "        if with_onehot:\n",
    "            return pred\n",
    "        return np.argmax(pred, axis=1)\n",
    "\n",
    "    def train(self, X_train, Y_train, X_val, Y_val, loss_func, hyper_params: dict, show_plot=False):\n",
    "        # X_train -> (n_samples, n_features)\n",
    "        # Y_train -> (n_samples, n_classes) one-hot \n",
    "        \n",
    "        # params = self.weight_init(self.params_set_list)\n",
    "\n",
    "        n_samples = X_train.shape[0]\n",
    "\n",
    "        # 將 train data 打包成 batch\n",
    "        X_batch_all, Y_batch_all = self.pack_to_batch(X_train, Y_train, hyper_params['batch_size'], n_samples)\n",
    "        \n",
    "        train_loss_arr = []\n",
    "        val_loss_arr = []\n",
    "        \n",
    "        val_acc_arr = []\n",
    "\n",
    "        for i in range(hyper_params['epoch']):\n",
    "            loss = 0\n",
    "            for X_batch, Y_batch in zip(X_batch_all, Y_batch_all):\n",
    "                # 單個 batch 訓練過程\n",
    "                # 1. 前向傳播\n",
    "                # 2. 反向傳播\n",
    "                # 3. 更新權重   \n",
    "                self.forward(X_batch)\n",
    "                self.backward(X_batch, Y_batch)\n",
    "                self.update_params({'lr': hyper_params['lr'], 'alpha': hyper_params['alpha']})\n",
    "                loss += loss_func.cal_loss(self.get_pred(X_batch, with_onehot=True), Y_batch)\n",
    "              \n",
    "            # print(\"Epoch: \", i)\n",
    "            # print('Loss:', round(loss, 2))\n",
    "\n",
    "            predictions = self.get_pred(X_val)\n",
    "            # print('Val Acc:', round(get_accuracy(predictions, Y_val), 2))\n",
    "            \n",
    "            train_loss_arr.append(loss / n_samples)\n",
    "\n",
    "            # 取 output layer 經過 activation function 的結果為 prediction\n",
    "            val_loss_arr.append(loss_func.cal_loss(self.get_pred(X_val, with_onehot=True), Y_val) / len(X_val))\n",
    "            val_acc_arr.append(self.calculate_acc(predictions, Y_val))\n",
    "\n",
    "        if show_plot:\n",
    "            self.plot_loss_acc(train_loss_arr, val_loss_arr, val_acc_arr)\n",
    "\n",
    "        return train_loss_arr, val_loss_arr, val_acc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel([Conv2d(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), \n\u001b[0;32m      8\u001b[0m                  LeckyReLU(),\n\u001b[0;32m      9\u001b[0m                  Flatten(),\n\u001b[0;32m     10\u001b[0m                  Linear(\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m     11\u001b[0m                  Softmax()])\n\u001b[0;32m     13\u001b[0m hyper_params \u001b[38;5;241m=\u001b[39m {    \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m     18\u001b[0m }\n\u001b[1;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m, in \u001b[0;36mMyModel.train\u001b[1;34m(self, X_train, Y_train, X_val, Y_val, loss_func, hyper_params, show_plot)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, Y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_batch_all, Y_batch_all):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# 單個 batch 訓練過程\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# 1. 前向傳播\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# 2. 反向傳播\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# 3. 更新權重   \u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: hyper_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: hyper_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[0;32m     62\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_func\u001b[38;5;241m.\u001b[39mcal_loss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pred(X_batch, with_onehot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), Y_batch)\n",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m, in \u001b[0;36mMyModel.backward\u001b[1;34m(self, X, label)\u001b[0m\n\u001b[0;32m     18\u001b[0m dLdZ \u001b[38;5;241m=\u001b[39m label\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(idx_maxlayer, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 倒敘遍歷\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     dLdZ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdLdZ\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Programs\\NumpyImplementNN\\my_nn_lib\\layers\\conv.py:52\u001b[0m, in \u001b[0;36mConv2d.backward\u001b[1;34m(self, delta)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, delta):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m#  back_prop_params: {'delta_next': delta_next, \u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#                     'w_next': w_next, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m#  如果 next layer 是 flatten\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m#  dLdZ -> (N, out_c*out_h*out_w)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     dLdZ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcal_dLdZ(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw)\n\u001b[1;32m---> 52\u001b[0m     dW, db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcal_dLdW\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_delta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dW\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_delta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m db\n",
      "File \u001b[1;32me:\\Programs\\NumpyImplementNN\\my_nn_lib\\layers\\conv.py:83\u001b[0m, in \u001b[0;36mConv2d.cal_dLdW\u001b[1;34m(self, x, delta)\u001b[0m\n\u001b[0;32m     81\u001b[0m diliated_stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     82\u001b[0m dilated_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilate(delta, diliated_stride, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m dLdW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilated_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# dLdW -> (out_c, in_c, kh, kw)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m dLdb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(delta, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[1;32me:\\Programs\\NumpyImplementNN\\my_nn_lib\\layers\\conv.py:148\u001b[0m, in \u001b[0;36mConv2d.convolution\u001b[1;34m(self, input_feat, filter, bias)\u001b[0m\n\u001b[0;32m    146\u001b[0m     out_feat \u001b[38;5;241m=\u001b[39m (im2col_feat \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m+\u001b[39m bias)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     out_feat \u001b[38;5;241m=\u001b[39m (\u001b[43mim2col_feat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# out_feat -> (out_c, N*out_h*out_w)\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# 將 w 重新 reshape 成 (out_c, in_c, kh, kw)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# 所以先將 (out_c, N*out_h*out_w) 拆成 (out_c, N, out_h, out_w) 後再 permute\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# out_feat -> (N, out_c, out_h, out_w)\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_feat\u001b[38;5;241m.\u001b[39mreshape(out_c, N, out_h, out_w)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 12)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# making some dummy data about 3 images with 3 channels and labels\n",
    "images = np.random.randint(0, 255, size=(4, 3, 5, 5)).astype(np.float32)\n",
    "labels = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]])\n",
    "\n",
    "model = MyModel([Conv2d(3, 3, (3, 3), 2, 0), \n",
    "                 LeckyReLU(),\n",
    "                 Flatten(),\n",
    "                 Linear(2*2*3, 3),\n",
    "                 Softmax()])\n",
    "\n",
    "hyper_params = {    \n",
    "    'lr': 0.01,\n",
    "    'epoch': 50,\n",
    "    'batch_size': 2,\n",
    "    'alpha': 0.9\n",
    "}\n",
    "model.train(images, labels, images, labels, CrossEntropyLoss, hyper_params, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 78\u001b[0m\n\u001b[0;32m     68\u001b[0m X_val, Y_val \u001b[38;5;241m=\u001b[39m X_data[train_size:], Y_data[train_size:]\n\u001b[0;32m     70\u001b[0m hyper_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m     75\u001b[0m }\n\u001b[1;32m---> 78\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMyModel\u001b[49m([Linear(\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m6\u001b[39m), \n\u001b[0;32m     79\u001b[0m                  LeckyReLU(),\n\u001b[0;32m     80\u001b[0m                  Linear(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m),\n\u001b[0;32m     81\u001b[0m                  Softmax()])\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# model = MyModel([Linear(4, 3), Softmax()])\u001b[39;00m\n\u001b[0;32m     85\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain(X_train, Y_train, X_val, Y_val, CrossEntropyLoss, hyper_params, show_plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyModel' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 固定隨機種子，確保同參數下每次結果都相同\n",
    "np.random.seed(1)\n",
    "\n",
    "def wine():\n",
    "    with open('data/winequality-red.csv') as f:\n",
    "        # 跳過 first row (標籤名稱)\n",
    "        data = f.readlines()[1:]\n",
    "\n",
    "    data = [line.strip().split(',') for line in data]\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = []\n",
    "\n",
    "    # 最後一個 column 為 label\n",
    "    classes = np.unique(data[:, -1])\n",
    "\n",
    "    # 將 label 做 one-hot encoding\n",
    "    for d in data:\n",
    "        for cls in classes:\n",
    "            if d[-1] == cls:\n",
    "                one_hot = np.zeros(len(classes))\n",
    "                one_hot[classes.tolist().index(cls)] = 1\n",
    "                labels.append(one_hot)\n",
    "    return data, labels\n",
    "\n",
    "def iris():\n",
    "    with open('data/Iris.csv') as f:\n",
    "    # 跳過 first row (標籤名稱)\n",
    "        data = f.readlines()[1:]\n",
    "\n",
    "    data = [line.strip().split(',')[1:] for line in data]\n",
    "    data = np.array(data)\n",
    "    labels = []\n",
    "\n",
    "    classes = np.unique(data[:, 4])\n",
    "\n",
    "    # 將 label 做 one-hot encoding\n",
    "    for d in data:\n",
    "        for cls in classes:\n",
    "            if d[4] == cls:\n",
    "                one_hot = np.zeros(len(classes))\n",
    "                one_hot[classes.tolist().index(cls)] = 1\n",
    "                labels.append(one_hot)\n",
    "    return data, labels\n",
    "\n",
    "# data, labels = iris()\n",
    "data, labels = wine()\n",
    "# 將 input features 與 labels 從原始資料中分離\n",
    "inputs = data[:, :-1].astype(np.float32)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 對 input features 做標準化\n",
    "inputs = (inputs - np.mean(inputs, axis=0)) / np.std(inputs, axis=0)\n",
    "\n",
    "# 打亂數據\n",
    "idx = np.random.permutation(len(inputs))\n",
    "\n",
    "X_data = inputs[idx]\n",
    "Y_data = labels[idx]\n",
    "\n",
    "# 設定 train set 和 val set 的比例 (80% train, 20% val)\n",
    "train_size = int(len(X_data) * 0.80)\n",
    "\n",
    "\n",
    "X_train, Y_train = X_data[:train_size], Y_data[:train_size]\n",
    "X_val, Y_val = X_data[train_size:], Y_data[train_size:]\n",
    "\n",
    "hyper_params = {\n",
    "    'lr': 0.01,\n",
    "    'epoch': 50,\n",
    "    'batch_size': 16,\n",
    "    'alpha': 0.9\n",
    "}\n",
    "\n",
    "\n",
    "model = MyModel([Linear(11, 6), \n",
    "                 LeckyReLU(),\n",
    "                 Linear(6, 6),\n",
    "                 Softmax()])\n",
    "\n",
    "# model = MyModel([Linear(4, 3), Softmax()])\n",
    "\n",
    "params = model.train(X_train, Y_train, X_val, Y_val, CrossEntropyLoss, hyper_params, show_plot=True)\n",
    "# mlp.kfold(X_data, Y_data, FOLD, SquareLoss, hyper_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:\n",
      " [[[[ 6.  0.  6.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 1.  0. 19.]]\n",
      "\n",
      "  [[16.  0. 12.]\n",
      "   [ 0.  0.  0.]\n",
      "   [18.  0. 18.]]\n",
      "\n",
      "  [[16.  0. 11.]\n",
      "   [ 0.  0.  0.]\n",
      "   [18.  0. 17.]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv = Conv2d(3, 3, (2, 2), 1, 1)\n",
    "a = np.random.randint(0, 20, size=(1, 3, 2, 2)).astype(np.float32)\n",
    "b = conv.dilate(a, 2, 0)\n",
    "# print('a:\\n', a)\n",
    "# print(b.shape)\n",
    "# b = np.sum(b, axis=(1, 2, 3))\n",
    "print('b:\\n', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(a, [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, constant_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print('a:\\n', a)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print('b:\\n', b)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(0, 20, size=(1, 3, 2, 2)).astype(np.float32)\n",
    "b = np.pad(a, [[1, 1] if i == 3 else [0, 0] for i in range(4)], 'constant', constant_values=0)\n",
    "# print('a:\\n', a)\n",
    "# print('b:\\n', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(np.ndarray):\n",
    "    def __new__(cls, input_array, requires_grad=False):\n",
    "        # 建立 ndarray 的子類\n",
    "        obj = np.asarray(input_array).view(cls)\n",
    "        obj.requires_grad = requires_grad\n",
    "        obj.grad = None  # 儲存梯度\n",
    "        obj._grad_fn = None  # 計算梯度的函數\n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj):\n",
    "        # 當新 Tensor 被創建時，繼承屬性\n",
    "        if obj is None: return\n",
    "        self.requires_grad = getattr(obj, 'requires_grad', False)\n",
    "        self.grad = getattr(obj, 'grad', None)\n",
    "        self._grad_fn = getattr(obj, '_grad_fn', None)\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        if not self.requires_grad:\n",
    "            raise RuntimeError(\"This tensor does not require gradients.\")\n",
    "        \n",
    "        if grad_output is None:\n",
    "            grad_output = np.ones_like(self)\n",
    "        \n",
    "        if self.grad is None:\n",
    "            self.grad = grad_output\n",
    "        else:\n",
    "            self.grad += grad_output\n",
    "\n",
    "        if self._grad_fn:\n",
    "            self._grad_fn(grad_output)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({super().__repr__()}, requires_grad={self.requires_grad})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(Tensor([[1, 2],\n",
       "        [3, 4]]), requires_grad=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([[1, 2], [3, 4]])\n",
    "a = Tensor(b)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Tensor"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opengl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
